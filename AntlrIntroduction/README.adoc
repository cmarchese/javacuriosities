= Antlr Introduction

* La idea de este proyecto es crear un simple parser de archivos CSV, para esto usaremos ANTLR 3 para aprender algunos conceptos básicos de esta herramienta, la cual nos permite definir una gramática que luego puede generar lexers y parsers en alguno de los lenguajes soportados, cabe aclarar que este parser no cumplirá con toda la https://tools.ietf.org/html/rfc4180[RFC-4180]

== Definiciones

Antes de continuar vamos a pensar en los distintos elementos que nos podemos encontrar en un archivo CSV
Dentro de un archivo CSV podemos tener varias líneas separadas por un salto de línea(CRLF), al mismo tiempo cada línea/fila puede tener valores separados por comas, aquí podemos detectar dos tipos de valores, los simples y los quoted.

Valor simple: O sea uno o mas caracteres distintos de coma, salto de línea y comillas
Valor quoted: Aca hablamos de valores formados por una comilla doble, seguido por cero o mas caracteres y terminados en comilla doble

== Terminales y No Terminales

Cuando hablamos de símbolos terminales nos referimos a aquellos que ya representa la mínima unidad en nuestro lenguaje, en cambio los no terminales son construidos como bloques de varios símbolos terminales, en nuestro parser tendremos los siguientes símbolos

Terminales:

* Coma
* Salto de línea
* Comillas dobles
* Otro carácter que no sea ni coma, salto de línea o comillas dobles

== Lexer Vs Parser

Dependiendo como realicemos nuestro parser deberemos o no crear un lexer, dado que estamos usando ANTLR tendremos dos fases distintas una para la parte léxica y otra la sintáctica.

Lexer: Podemos definir al lexer como el encargado de procesar el input y separarlo en distintos tokens, en general el lexer ira leyendo el input y obteniendo tokens (basado en expresiones regulares) y se los pasara al parser.

Parser: Los parser van un nivel mas y toman los token producidos por el lexer y tratan de determinar si las sucesión de tokens provista es valida, esto lo hace por medio de una gramática.

== Expresiones regulares

Las expresiones regulares o regex, son secuencias de caracteres que forman un patron de búsqueda, además pueden contener caracteres que tengan un significado especial, estos se conocen como metacarácteres o metasímbolos, si queremos usarlos debemos también agregar un carácter de escape que "desactiva" el significado especial. 
Es importante que existen tres operaciones básicas

Alternativas: Si r y s son expresiones regulares entonces r|s es una expresión que es valida cuando la cadena concuerda con r o con s, esto se conoce como union L(r|s) = L(r) ∪ L(s)

Concatenación: Teniendo en cuenta r y s, podemos decir que rs es correspondiente a la concatenación de dos cadenas donde la primera valida r y la segunda valida s. L(rs) = L(r) L(s)

Repetición: Esta operación también es conocida como cerradura (de Kleene) y se escribe r*, donde r es valida para cualquiera de las cadenas ε (Cadena vacia), r, rr, rrr, .... La representación de esta operación es r* = {ε} ∪ {r} ∪ {rr} ∪ ...
 
=== Cuadro de metacarácteres en ANTLR

[options="header"]
|===
|Carácter	|Significado	|Ejemplo		|Explicación
|\|		 	|Or Lógico		|'a'\|'b'		|Es valida cuando la cadena es 'a' o 'b' 
|?		 	|Opcional		|'a''b'?		|Es valida cuando la cadena es 'a' o 'ab'
|*		 	|Cero o mas		|'a'*			|Es valida cuando la cadena es vacía, 'a', 'aa', 'aaa', ...
|+		 	|Uno o mas		|'a'+			|Es valida cuando la cadena es 'a', 'aa', 'aaa', ...
|~		 	|Negación		|~('a'\|'b')	|Es valida cuando la cadena esta formada de cualquier carácter entre \u0000..\uFFFF menos 'a' o 'b'
|(...)	 	|Grupos			|('a' 'b')+		|Es valida cuando la cadena es 'ab', 'abab', 'ababab', ...
|===

== Hands on - Lexer

Vamos a empezar construyendo el lexer de nuestro proyecto, para esto vamos a ir al archivo CSVLexer.g

=== Lexer Reglas 

Una forma simple de entender el algoritmo aplicado por el Lexer es teniendo en cuenta tres reglas

* First Longest Matching Rule

Si dos reglas del lexer coinciden con el input se selecciona la mas larga.

CHAR: 'a'
SHORT: 'abc'
LONG: 'abcabc'

** Si ingresamos el input "abc", se seleccionara el token SHORT porque coincide exactamente 
** Si ingresamos el input "abcabc", se seleccionara el token LONG y no dos token SHORT
** Si ingresamos el input "abca", se seleccionara el token LONG y se marcara error porque es un token incompleto

* Greedy Token Matching

Por defecto el lexer intentara consumir la mayor cantidad posibles para un determinado token, pero en ciertas ocasiones vamos a querer cambiar esto.

COMMENT: '/*' (.)* '*/'

Aquí vemos una regla que indica que los comentarios tienen cualquier contenido entre los delimitadores, pero el (.)* genera un ciclo en Antlr los cuales siempre intentan consumir el carácter para continuar su ejecuccion y nunca seríamos capaces de detectar el '*/', para esto podemos usar la opción greedy

COMMENT: '/*' ( options {greedy=false;}: .)* '*/'

Ahora antes de iniciar una nueva iteración se comprobara si se puede finalizar ese token.

* Decision Finality

Esta regla indica que luego que el lexer selecciono un token no se arrepiente de su elección e intentara matchear su contenido sin hacer ningún tipo de backtracking, algunas veces podemos
implicar una especie de backtracking selectivo por medio de un predicado sintáctico.

SHORT: 'aaa'
LONG: 'aaaa'

** Si ingresamos el input "aaaa", se seleccionara el token LONG aunque si vemos es totalmente valido tener dos token SHORT y convertir todo el input en tokens