= IO Models
:toc: macro
:sectnumlevels: 4

La idea de este documento es revisar los distintos modelos de IO.

toc::[]
== Why?

Antes de empezar a entender los distintos modelos de IO, seria bueno saber por que
podríamos estar interesado en ellos.

Cuando desarrollamos una aplicación, esta realiza distintas tareas, realizar cálculos,
leer archivos, enviar emails, etc. Dependiendo del tipo de tarea que ejecutemos podremos
decir que nuestro proceso es *CPU-bound* o *I/O-bound*.

=== CPU-bound vs I/O-bound

Se dice que nuestro proceso es CPU-bound si el bottlenecked es el CPU o IO-bound
si el bottlenecked son las operaciones de IO, como leer o escribir archivos o operaciones
de networking.

Cuando nos encontramos desarrollando aplicaciones IO-bound nos resulta util conocer y entender
los distintos modelos de IO. Por ejemplo podríamos analizar el caso de cualquier
servidor de alta performance (Netty, Grizzly, etc).

== IO

Si pensamos en algún servidor web, veremos que la parte de IO resulta fundamental a la hora
de manejar multiples requests. Estamos leyendo request data por medio de la red, enviando el
response para esa request e incluso leyendo archivos desde disco para servir contenido estático.

*Es util pensar a las operaciones de IO en dos etapas*

* No hay datos o están en el controller -> Los *_datos están disponibles_* para el kernel.
* Datos en los buffers del kernel -> Datos en la memoria de la aplicación.

*La primera etapa de chequeo del dispositivo puede ser blocking o non-blocking, y la segunda de transmicion de
los datos puede ser synchronous o asynchronous.*

Otra aclaración importante es que significa que los *_datos están disponibles_* y esto va
a depender de la información que estemos leyendo.

* UDP: Un datagrama entero fue recibido.
* TCP: Datos recibidos que desde la ultima watermark.

=== User mode vs Kernel mode

Un proceso ejecuta instrucciones en dos modos distintos, ya sea a nivel de aplicación o a nivel de kernel, ya
que ciertas acciones necesitan mas privilegios que otras. Esto para el usuario es transparente pero hacer el switch
de un modo al otro es costo.

* *Kernel mode:* El código que se esta ejecutando tiene acceso completo e ilimitado al hardware subyacente.
Este puede ejecutar cualquier instrucción y referenciar cualquier dirección de memoria. Este modo es reservado
funciones de bajo nivel del sistema operativo. Cualquier crash en este nivel seria catastrófico y detendría la PC.

* *User Mode:* El código que se esta ejecutando no tiene la capacidad de acceder a hardware o memoria sin delegar estas
acciones en una llamado a un API del sistema (system call). Debido a esta separación los crash a este nivel son
recuperables, la mayoría del código se ejecuta en este nivel.

image::images/ProtectionRings.png[]

== Models

Iremos explicando cada modelo junto con una imagen y las descripción de lo que sucede.

_Nota: Las imágenes son de un ejemplo de socket UDP ya que el concepto de data being "ready" es mas simple_

=== 1. Blocking I/O

El modelo más frecuente de IO es el blocking IO, este es el utilizado por las clases del paquete java.io.
Además todos los sockets son blocking por defecto.

image::images/BlockingIO.png[]

En la imagen se ve que el proceso llama a _recvfrom_ y la system call no retorna hasta que el datagrama llega y es
copiado al buffer de la aplicación o ocurre un error.
Decimos que nuestro proceso esta _blocked_ durante todo el tiempo hasta que la system call retorna.

=== 2. Non Blocking IO

Cuando realizamos una operación de IO, esta se encuentra asociada a un FD (File Descriptor) el cual podemos configurar
para ser Non-Blocking, de esta forma le estamos indicando al kernel que _"Cuando una operación de IO no pueda ser completada arroje un error
en lugar de pasar al proceso al estado de sleep"_

image::images/NonBlockingIO.png[]

* Para los tres primeros llamados a _recvfrom_ no hay datos y el kernel retorna de forma inmediata con el error _EAGAIN o EWOULDBLOCK_.
* Para el cuarto llamado, el datagrama esta listo y es copiado en el buffer de la aplicación, ahora _recvfrom_ retorna de forma exitosa. Luego los datos son procesados.

Cuando una aplicación se encuentra en un loop invocando alguna operación nonblocking sin obtener datos, esto se lo conoce como pooling (busy waiting).
La aplicación esta continuamente haciendo polling para ver si el kernel le indica que hay datos listos. Esto generalmente es un desperdicio de CPU, aunque puede ser
util en algunos escenarios específicos.

=== 3. I/O Multiplexing (select/poll/epoll/kqueue)

==== What is I/O Multiplexing?

Es la capacidad para indicarle al kernel que queremos ser notificados si uno o mas operaciones de IO están listas.
Algunas funciones como select, poll, epoll (linux) and kqueue (bsd) nos proveen esta funcionalidad.

==== Why Multiplexing?

Cuando una aplicación necesita manejar multiples descriptores de IO al mismo tiempo. e.g. file y socket descriptors o multiples socket descriptors.

==== I/O Multiplexing

Cuando usamos IO Multiplexing podemos utilizar alguna de las funciones provistas por el kernel (select, poll, epoll, kqueue) y hacer el block
en una de esas operaciones, en lugar de la system call de IO. +
En lugar de trabajar sobre un único file descriptor, podemos monitorear cambios en varios de ellos.

image::images/IOMultiplexing.png[]

En el ejemplo de arriba hacemos el block en el llamado al _select_, esperando a que el datagrama esta listo para ser leído.
Cuando el _select_ retorna esto indica que el socket es "readable", entonces podemos llamar a _recvfrom_ para copiar el datagrama
en el buffer de nuestra aplicación.

===== Comparing to the blocking I/O model
Si comparamos la figura del modelo 1 y la del 3:

* Ventajas: Podemos monitorear mas de un descriptor hasta que este listo
* Desventajas: Usando el _select_ requiere dos system calls _(select y recvfrom)_ en lugar de un solo llamado.

===== Multithreading with blocking I/O

Otro modelo muy relacionado es el de _Multithreading con blocking IO_. Ese modelo se asemeja mucho al anterior, excepto que en lugar de usar
_select_ para bloquear multiples descriptores utiliza multiples hilos, o sea uno por descriptor.


=== 4. Signal-Driven I/O Model (SIGIO)

También podemos decirle al kernel que nos notifique por medio de la señal _SIGIO_ cuando el descriptor esta listo.

image::images/SignalDrivenIO.png[]

* Primero instruimos al socket para enviar señales e instalamos un _signal handler_ por medio de la system call _sigaction_.
El retorno de este system call es inmediato y nuestro proceso continua; esto no es bloqueante.

* Cuando el datagrama esta listo, la señal _SIGIO_ es generada para nuestro proceso. Podemos:

** Leer el datagrama desde el _signal handler_ llamando a _recvfrom_ y luego notificar al loop principal que los datos estan listo en el buffer de la aplicación.

** Notificar al loop principal y dejar que este lea el datagrama.

Sin importar como manejamos la señal, la ventaja de este modelo es que no estamos bloqueados mientras esperamos que el datagrama este llegue. El loop
principal puede continuar ejecutando y solo esperar a ser notificado por el _signal handler_ que hay mas datos para ser procesados o leídos.

=== 5. Asynchronous I/O (AIO)

*Asynchronous I/O* esta definido en la especificación POSIX _(Portable Operating System Interface)_. La idea es indicarle al kernel
que comience una operación de IO y que este nos notifique cuando toda la operación haya finalizado (Incluyendo la copia de los datos
del buffer del kernel a la aplicación).

+++<u>La principal diferencia entre este modelo y el <i>signal-driven I/O</i>, es que el kernel nos indica que la operación de IO puede ser iniciada, pero con AIO el kernel nos indica que la operación esta completa.</u>+++

image::images/AsynchronousIO.png[]

* En este ejemplo llamamos a _aio_read_ (Las funciones del standard POSIX para asynchronous I/O empiezan con aio_ o lio_) y le envía al kernel las siguientes cosas:

** descriptor, buffer pointer, buffer size (Los mismos argumentos que le enviamos al _read_).
** file offset (Similar a _lseek_).
** Cómo notificarnos cuando se complete toda la operación.

* En este ejemplo asumimos que el kernel va a generar una señal para indicarnos que la operación esta completa. Esta señal no es generada hasta que los datos hayan sido copiados al buffer de la aplicación, lo cual es distinto al _signal-driven I/O model_.

*Nota:* Al parecer pocos SO soportan POSIX asynchronous I/O. Al parecer es mas común verlo en disk IO en lugar de un uso para sockets.

== Comparison of the I/O Models

image::images/ComparisonIO.png[]

La principal diferencia entre los primeros cuatro modelos es la primera fase, ya que la segunda fase en los primeros cuatro modelos es la misma: el proceso se bloquea en una llamada a _recvfrom_ mientras los datos se copian desde el kernel al buffer de la aplicación. Sin embargo, Asynchronous I/O maneja ambas fases y esto es distinto a los primeros cuatro modelos.

=== Synchronous I/O vs Asynchronous I/O

POSIX define estos dos términos de la siguiente manera:

* *Synchronous:* Si la operación de IO es sincrónica el proceso esta bloqueado hasta que la operación de IO este completa.
* *Asynchronous:* Si la operación de IO es asincrónica el proceso no queda bloqueado esperando por el fin de la operación.

Usando estas definiciones, los cuatros primeros modelos (Blocking, Non Blocking, I/O Multiplexing y Signal-Driven I/O) son sincrónicos, porque la llamada a _recvfrom_ bloquea el proceso. Solo el ultimo modelo se considera asincrónico.

== Comparison of select/poll/epoll

image::images/ComparisonMultiplexing.png[]

=== select()

----
fd_set fd_in, fd_out;
struct timeval tv;

// Reset the sets
FD_ZERO( &fd_in );
FD_ZERO( &fd_out );

// Monitor sock1 for input events
FD_SET( sock1, &fd_in );

// Monitor sock2 for output events
FD_SET( sock2, &fd_out );

// Find out which socket has the largest numeric value as select requires it
int largest_sock = sock1 > sock2 ? sock1 : sock2;

// Wait up to 10 seconds
tv.tv_sec = 10;
tv.tv_usec = 0;

// Call the select
int ret = select( largest_sock + 1, &fd_in, &fd_out, NULL, &tv );

// Check if select actually succeed
if ( ret == -1 )
    // report error and abort
else if ( ret == 0 )
    // timeout; no event detected
else
{
    if ( FD_ISSET( sock1, &fd_in ) )
        // input event on sock1

    if ( FD_ISSET( sock2, &fd_out ) )
        // output event on sock2
}
----

=== poll()

----
// The structure for two events
struct pollfd fds[2];

// Monitor sock1 for input
fds[0].fd = sock1;
fds[0].events = POLLIN;

// Monitor sock2 for output
fds[1].fd = sock2;
fds[1].events = POLLOUT;

// Wait 10 seconds
int ret = poll( &fds, 2, 10000 );
// Check if poll actually succeed
if ( ret == -1 )
    // report error and abort
else if ( ret == 0 )
    // timeout; no event detected
else
{
    // If we detect the event, zero it out so we can reuse the structure
    if ( pfd[0].revents & POLLIN )
        pfd[0].revents = 0;
        // input event on sock1

    if ( pfd[1].revents & POLLOUT )
        pfd[1].revents = 0;
        // output event on sock2
}
----

=== epoll()/kqueue()

----
// Create the epoll descriptor. Only one is needed per app, and is used to monitor all sockets.
// The function argument is ignored (it was not before, but now it is), so put your favorite number here
int pollingfd = epoll_create( 0xCAFE );

if ( pollingfd < 0 )
 // report error

// Initialize the epoll structure in case more members are added in future
struct epoll_event ev = { 0 };

// Associate the connection class instance with the event. You can associate anything
// you want, epoll does not use this information. We store a connection class pointer, pConnection1
ev.data.ptr = pConnection1;

// Monitor for input, and do not automatically rearm the descriptor after the event
ev.events = EPOLLIN | EPOLLONESHOT;
// Add the descriptor into the monitoring list. We can do it even if another thread is
// waiting in epoll_wait - the descriptor will be properly added
if ( epoll_ctl( epollfd, EPOLL_CTL_ADD, pConnection1->getSocket(), &ev ) != 0 )
    // report error

// Wait for up to 20 events (assuming we have added maybe 200 sockets before that it may happen)
struct epoll_event pevents[ 20 ];

// Wait for 10 seconds
int ret = epoll_wait( pollingfd, pevents, 20, 10000 );
// Check if epoll actually succeed
if ( ret == -1 )
    // report error and abort
else if ( ret == 0 )
    // timeout; no event detected
else
{
    // Check if any events detected
    for ( int i = 0; i < ret; i++ )
    {
        if ( pevents[i].events & EPOLLIN )
        {
            // Get back our connection pointer
            Connection * c = (Connection*) pevents[i].data.ptr;
            c->handleReadEvent();
         }
    }
}
----